# mipt_ml_hw05_6sem
Задача заключалась в исследовании использования глубоких нейронных сетей для решения проблемы распознавания лиц. Необходимо обучить модель, которая способна идентифицировать людей по набору изображений лиц.

Проект выполнен студентами 3 курса МФТИ ПИШ ФАЛТ:
* Ищенко Игорем Олеговичем
* Нурыевой Айнур
* Шатуновым Евгением Вячеславовичем
____
# Описание задачи

Реализация алгоритма, который способен по переданному ему для запоминания изображению с лицом человека определить, находится ли этот же человек на других изображениях, переданных модели.
В качестве датасета был выбран [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/), а модели для обучения [FaceNet](https://github.com/davidsandberg/facenet).

В качестве задания требовалось:
* Проанализировать имеющиеся датасеты и выбрать наиболее подходящий
* Выбрать архитектуру модели для обучения
* Обучить модель и оценить полученные результаты
* Произвести инференс
* Добавить логирование wandb
* Добавить сохранения весов модели
* Добавить тесты к исходному коду в частях: проверка датасета, проверка пайплайна обучения, проверка метрики и функции ошибки.
* Добавить пайплайн dvc
* Написать make команды для запуска отдельных этапов выполнения кода
* Добавить описание процесса работы над проектом
____
# Рабочий процесс

* **Весь контроль над процессом работы вёлся с помощью Яндекс Трекера.**

    Был создан проект с расписанными задачами, начиная
    от переноса кода по модулям, заканчивая выполнением всех обязательных и дополнительных задач.

    <img src="./docs/tracker_tasks.png" width="600" height="350">


* **Использовалась Agile методика.**

    Проводилось покер-планирование, группы задач были разбиты по 3 спринтам.

    <img src="./docs/tracker_board.png" width="600" height="350">


* **Для удобной работы в репозитории было создано соглашение о наименовании коммитов, веток и MR.**
  * Коммиты: <тип>:<изменение>
  * Ветки: <тип>/<область работы>-<цель работы>
  * MR: <тип>/<область работы>-<цель работы> со ссылкой на задачу из трекера.


* **С целью контроля качества кода и работы с git использовались прекоммиты.**

    Подробную конфигурацию можно найти в файле: ```.pre-commit-config.yaml```

    Туда были добавлены:
  * ruff для форматирования и линта кода
  * Проверка YAML и JSON файлов
  * Контроль сообщения коммитов
  * Проверка конфликтов слияния


* **Для проверки вливаемого кода был написан пайплайн CI/CD.**

    CI/CD совершает сборку кода, проверяет формат, запускает линтер и тесты.
    Полный пайплайн находится здесь: ```.github/workflows/.github-ci.yaml```


* **Важным принципом работы был запрет на вливание в main без проверки кода всеми членами команды.**

    Такой подход позволил лучше познакомиться с кодом всего проекта каждым его участником.


* **Для комфортной работы с окружением был выбран Poetry.**

    Poetry помог проще настроить систему тестирования, управлять путями к написанным программным модулям,
    подобрать подходящие версии библиотек и настроить использование автоформата кода.


* **Была написана система тестов, покрывающая весь код на 98%.**

    Минимальным требованием был выбран порог 80%.


* **Команда тщательно подходила к документации и доработкам исходного кода.**

    Все функции были задокументированы, а сложные моменты в их реализации дополнительно пояснены.
    Найденные недочёты были исправлены, код был сделан более читаемым там, где это было необходимо.
____
# Распределение задач
### Ищенко Игорь
* Добавление make команд для запуска отдельных частей кода
* Добавление пайплайна dvc
* Тестирование основных этапов выполнения кода
* Добавление логирования wandb

### Нурыева Айнур
* Написание кода загрузки датасета
* Обучение модели
* Настройка окружения проекта
* Написание инференса
* Создание и настройка репозитория

### Шатунов Евгений
* Исследование имеющихся датасетов
* Исследование моделей для решения задачи
* README: Описание запуска и процесса работы
* Подготовка трекера к работе
____
# Установка окружения
Сперва установите `poetry`
```bash
pip install poetry
```

Для создания окружения и установки всех нужных пакетов запустите
```bash
make install
```

Для установки хуков
```bash
make hooks
```

Для запуска линтеров
```bash
make lint
```

Для запуска тестов
```bash
make test
```

Для обновления и фиксации зависимостей
```bash
poetry lock
```

Для активации виртуального окружения скопируйте то, что выдает эта команда и запустите скопированную команду
```bash
poetry env use python3.12
poetry env activate
```

Для установки данных для обучения
```bash
make install_dataset
```

Для запуска JupyterLab (нужно запустить внутри настроенного окружения)
```bash
jupyter lab
```

# Запуск пайплайна

При первом запуске, используйте сперва:
```bash
dvc init
```
После этого воспользуйтесь командой:
```bash
dvc repro
```

# Веса модели
Скачать веса модели можно отсюда
```
https://disk.yandex.ru/d/P8FJEn3CUMJbnQ
```

# Отчёт о модели
Демонстрацию работы модели можно посмотреть в папке face-recognition.
Графики обучения: https://api.wandb.ai/links/nuryyevva-mipt/yotuu7se.
# Структура проекта

```
project_root/
├── .git/                    # Git repository (managed by Git)
├── .dvc/                    # DVC metadata directory (managed by DVC)
├── .wandb/                  # Weights & Biases local data (often hidden)
├── src/                     # Source code directory
│   ├── __init__.py
│   ├── model/               # Transformer model src code
│   │   ├── __init__.py
│   │   ├── *.py             # Class definitions, architectures
│   ├── data/                # Data processing and loading
│   │   ├── __init__.py
│   │   ├── prepare_data.py  # Data preprocessing
│   ├── utils/               # Utility functions and modules
│   │   ├── __init__.py
│   │   ├── *.py
│   ├── main.py
│   ├── config.py            # Config file
│
├── data/
│   │── data
│
├── model/                   # Trained model weights
│   │   ├── model.pt         # PyTorch weights
│
├── notebooks/               # Jupyter notebooks for demonstration
│
├── tests/                   # Tests directory
│   ├── __init__.py
│
```
